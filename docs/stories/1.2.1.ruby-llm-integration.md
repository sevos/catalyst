# Story 1.2.1: RubyLLM Integration

## Status
Draft

## Story
**As a** Rails Developer,
**I want** the Catalyst framework to integrate RubyLLM as its LLM provider layer,
**so that** I can leverage a battle-tested, Rails-native solution for LLM interactions with support for multiple providers.

## Acceptance Criteria
1. RubyLLM gem is added as a dependency to the Catalyst engine
2. The `rails g catalyst:install` generator creates a RubyLLM configuration initializer
3. The initializer prompts for and securely stores API keys using Rails credentials
4. Catalyst::Execution model integrates with RubyLLM's `acts_as_chat` functionality
5. Basic RubyLLM configuration works with at least OpenAI provider
6. Generator output clearly indicates next steps for API key configuration

## Tasks / Subtasks

- [ ] Add RubyLLM dependency (AC: 1)
  - [ ] Add `ruby_llm` to catalyst.gemspec
  - [ ] Bundle and verify installation
  - [ ] Document minimum version requirements
- [ ] Update install generator (AC: 2, 3, 6)
  - [ ] Add RubyLLM initializer template
  - [ ] Implement API key prompting logic
  - [ ] Store keys in Rails credentials
  - [ ] Add clear post-install instructions
- [ ] Integrate Execution model (AC: 4)
  - [ ] Add `acts_as_chat` to Catalyst::Execution
  - [ ] Run necessary migrations for chat fields
  - [ ] Verify message persistence works
- [ ] Configure OpenAI provider (AC: 5)
  - [ ] Set up default OpenAI configuration
  - [ ] Test basic chat functionality
  - [ ] Add error handling for missing credentials
- [ ] Update documentation
  - [ ] Add RubyLLM setup to README
  - [ ] Document supported providers
  - [ ] Include troubleshooting guide

## Dev Notes

**Relevant Source Tree info:**
- Gemspec location: `/catalyst.gemspec`
- Install generator: `/lib/generators/catalyst/install/install_generator.rb`
- Execution model: `/app/models/catalyst/execution.rb`
- Generator templates: `/lib/generators/catalyst/install/templates/`

**Important notes:**
- This story must be completed before Story 1.3
- RubyLLM version should be ~> 1.3 for Rails 8 compatibility
- Consider future multi-provider support in initializer design
- Ensure credentials are namespaced under `:catalyst` key

### Implementation Approach

1. **Dependency Addition**:
   ```ruby
   # catalyst.gemspec
   spec.add_dependency "ruby_llm", "~> 1.3"
   ```

2. **Initializer Template**:
   ```ruby
   # lib/generators/catalyst/install/templates/catalyst.rb.tt
   RubyLLM.configure do |config|
     config.openai_api_key = Rails.application.credentials.dig(:catalyst, :openai_api_key)
     # Future providers can be added here
   end
   ```

3. **Execution Model Update**:
   ```ruby
   module Catalyst
     class Execution < ApplicationRecord
       acts_as_chat
       belongs_to :agent
     end
   end
   ```

### Testing
- Test file location: `/test/generators/catalyst/install_generator_test.rb`
- Test standards: Verify initializer generation, credential storage, model integration
- Testing frameworks: Rails minitest
- Specific requirements: 
  - Test initializer is created correctly
  - Test credentials are properly namespaced
  - Test Execution model has chat capabilities
  - Mock API calls in tests

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-07-10 | 1.0 | Initial story creation based on architect research | Winston (Architect) |

## Dev Agent Record

### Agent Model Used
*To be populated by development agent*

### Debug Log References
*To be populated by development agent*

### Completion Notes List
*To be populated by development agent*

### File List
*To be populated by development agent*

## QA Results
*To be populated by QA agent*