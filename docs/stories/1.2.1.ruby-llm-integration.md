# Story 1.2.1: RubyLLM Integration

## Status
Draft

## Story
**As a** Rails Developer,
**I want** the Catalyst framework to integrate RubyLLM as its LLM provider layer,
**so that** I can leverage a battle-tested, Rails-native solution for LLM interactions with proper configuration and execution tracking.

## Acceptance Criteria
1. RubyLLM gem is added as a dependency to the Catalyst engine
2. The `rails g catalyst:install` generator creates a comprehensive RubyLLM configuration initializer
3. The initializer reads API keys from environment variables with Rails credentials fallback
4. Catalyst::Execution model has chat-like fields for tracking interactions without storing message content
5. Default configuration uses OpenAI as provider with gpt-4.1-nano as default model
6. Execution model stores input parameters (text + serialized JSON) similar to Agent model_params pattern
7. Generator output clearly indicates next steps for API key configuration

## Tasks / Subtasks

- [ ] Add RubyLLM dependency (AC: 1)
  - [ ] Add `ruby_llm` to catalyst.gemspec
  - [ ] Bundle and verify installation
  - [ ] Document minimum version requirements
- [ ] Update install generator (AC: 2, 3, 7)
  - [ ] Add comprehensive RubyLLM initializer template
  - [ ] Configure environment variables with Rails credentials fallback
  - [ ] Set OpenAI as default provider with gpt-4.1-nano model
  - [ ] Add clear post-install instructions
- [ ] Enhance Execution model (AC: 4, 6)
  - [ ] Add chat-like fields: `interaction_count`, `last_interaction_at`
  - [ ] Add `input_params` field with JSON serialization (similar to Agent model_params)
  - [ ] Create migration for new fields:
    - `add_column :catalyst_executions, :interaction_count, :integer, default: 0, null: false`
    - `add_column :catalyst_executions, :last_interaction_at, :datetime`
    - `add_column :catalyst_executions, :input_params, :text` (SQLite compatible, serialized with JSON)
  - [ ] Add helper methods for parameter management
- [ ] Configure default RubyLLM settings (AC: 5)
  - [ ] Set up OpenAI as default provider
  - [ ] Configure gpt-4.1-nano as default model
  - [ ] Add connection timeout and retry settings
  - [ ] Include logging configuration
- [ ] Update documentation
  - [ ] Add RubyLLM setup instructions
  - [ ] Document configuration options
  - [ ] Include troubleshooting guide

## Dev Notes

**Relevant Source Tree info:**
- Gemspec location: `/catalyst.gemspec`
- Install generator: `/lib/generators/catalyst/install/install_generator.rb`
- Execution model: `/app/models/catalyst/execution.rb`
- Generator templates: `/lib/generators/catalyst/install/templates/`

**Important notes:**
- This story must be completed before Story 1.3
- RubyLLM version should be ~> 1.3 for Rails 8 compatibility
- Do not use `acts_as_chat` or `acts_as_message` to avoid namespace pollution
- Execution model should mimic chat schema without storing actual messages
- Default provider: OpenAI, Default model: gpt-4.1-nano
- Ensure credentials are namespaced under `:catalyst` key

### Implementation Approach

1. **Dependency Addition**:
   ```ruby
   # catalyst.gemspec
   spec.add_dependency "ruby_llm", "~> 1.3"
   ```

2. **Initializer Template**:
   ```ruby
   # lib/generators/catalyst/install/templates/ruby_llm.rb.tt
   RubyLLM.configure do |config|
     # OpenAI Configuration (Default Provider)
     config.openai_api_key = ENV.fetch('OPENAI_API_KEY', nil) || 
                             Rails.application.credentials.dig(:catalyst, :openai_api_key)
     
     # Default Models
     config.default_model = 'gpt-4.1-nano'
     config.default_embedding_model = 'text-embedding-3-small'
     
     # Connection Settings
     config.request_timeout = 120
     config.max_retries = 3
     
     # Additional providers (configure as needed)
     config.anthropic_api_key = ENV.fetch('ANTHROPIC_API_KEY', nil) || 
                                Rails.application.credentials.dig(:catalyst, :anthropic_api_key)
     config.gemini_api_key = ENV.fetch('GEMINI_API_KEY', nil) || 
                             Rails.application.credentials.dig(:catalyst, :gemini_api_key)
     
     # Logging Configuration
     config.log_file = Rails.root.join('log/ruby_llm.log') if defined?(Rails)
     config.log_level = :info
   end
   ```

3. **Execution Model Update**:
   ```ruby
   module Catalyst
     class Execution < ApplicationRecord
       # JSON serialization for SQLite compatibility
       serialize :input_params, coder: JSON
       
       belongs_to :agent
       
       # Chat-like fields for tracking interactions (no message content)
       validates :interaction_count, presence: true, numericality: { greater_than_or_equal_to: 0 }
       
       # Helper methods for input parameter management
       def input_parameters
         input_params || {}
       end
       
       def input_parameters=(params)
         self.input_params = params
       end
       
       # Helper method to get specific input parameter
       def input_parameter(key)
         input_parameters[key.to_s]
       end
       
       # Helper method to set specific input parameter
       def set_input_parameter(key, value)
         current_params = input_parameters.dup
         current_params[key.to_s] = value
         self.input_parameters = current_params
       end
       
       # Update interaction tracking
       def increment_interaction!
         self.interaction_count = (interaction_count || 0) + 1
         self.last_interaction_at = Time.current
         save!
       end
     end
   end
   ```

### Testing
- Test file locations: 
  - Generator tests: `/test/generators/catalyst/install_generator_test.rb`
  - Model tests: `/test/models/catalyst/execution_test.rb`
- Test standards: Verify initializer generation, credential storage, model integration
- Testing frameworks: Rails minitest
- Specific requirements: 
  - Test initializer is created correctly with comprehensive configuration
  - Test environment variables and Rails credentials fallback
  - Test Execution model has chat-like fields and parameter handling
  - Test interaction tracking methods
  - Use RubyLLM test mode with `RubyLLM.test_mode!` and `RubyLLM.mock_response()`

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-07-10 | 1.0 | Initial story creation based on architect research | Winston (Architect) |
| 2025-07-10 | 1.1 | Refined based on PO feedback - removed acts_as_chat, added comprehensive config | Sarah (PO) |
| 2025-07-10 | 1.2 | Validation fixes - updated architecture alignment, test locations, migration specs | Sarah (PO) |
| 2025-07-10 | 1.3 | SQLite compatibility - updated migration to use text type with JSON serialization | Sarah (PO) |

## Dev Agent Record

### Agent Model Used
*To be populated by development agent*

### Debug Log References
*To be populated by development agent*

### Completion Notes List
*To be populated by development agent*

### File List
*To be populated by development agent*

## QA Results
*To be populated by QA agent*